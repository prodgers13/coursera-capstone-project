https://www.coursera.org/learn/applied-data-science-capstone/supplement/SDDeN/course-introduction
As a starting point of almost all data science projects, you need to collect data, as much and relevant as possible.
You will be collecting data from various sources. After your raw data has been collected, you will need to improve the quality by performing data wrangling. 
Then you can start exploring the processed data.  We will be your guide as we explore some really interesting real-world datasets together. You'll get to practice your SQL skills as we query the data and gather insights.  
You'll gain further insights into the data by applying some basic statistical analysis and data visualization, you'll be able to see directly how variables might be related to each other.   
We'll drill down into finer levels of detail by splitting the data into groups defined by categorical variables or factors in your data.  
You will be guided to build, evaluate, and refine predictive models for discovering more exciting insights. 
The final task of this capstone project is to create a presentation that will be developed into stories of all your analysis.
------------------------
Now install Anaconda.
	sh ~/Downloads/Anaconda3-2021.05-Linux-x86_64.sh
click yes a lot.

Installed anaconda, now I need to start it:
    source ~/anaconda3/etc/profile.d/conda.sh
    conda init
    conda update conda
    conda create -n notebooks jupyterlab
    conda activate notebooks
    jupyter lab
Works.
-------------------------------------
Project overview
    SpaceY run by Allon Mask 
-----------------
Integration with github 
    https://github.com/prodgers13/Capstone-Project.git    
--------------------
Data Collection Overview
    Spacex REST API https://api.spacexdata.com/v4/launches/past 
        url = "https://api.spacexdata.com/v4/launches/past"
        response = reqeusts.get(url)
        response.json() # a JSON object is returned 
        data = pd.json_normalize(response.json()) # convert it to a dataframe 
    Webscraping wiki web pages with BeautifulSoup 
--------------------
Importing notebook into Watson Studio   
    https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/labs/module_1_L2/jupyter-labs-spacex-data-collection-api.ipynb
-------------------------------
Data Wrangling Overview    
    I was stuck at some point because I could not figure out how to create a new column in the data frame for success or failure. I tried the dummies variable approach, but that wasn't quite right. 
    Task definition:
    TASK 4: Create a landing outcome label from Outcome column df.['Outcome']
Using the Outcome, create a list where the element is zero if the corresponding row in Outcome is in the set bad_outcome; otherwise, it's one. Then assign it to the variable landing_class.
This is what I came up with. I never understood how to iterate over a data froame. They key must be in the statement
    for row in df['must be the key to the dataframe']:
But that failed until I discovered that I had to skip the last one for some reason. These two problems held me up for about 5 hours on Wednesday into Thursday. The solution:

    landing_outcomes = df["Outcome"].value_counts()
    bad_outcomes=set(landing_outcomes.keys()[[1,3,5,6,7]])
    landing_class = []
    for row in df['FlightNumber']-1:
        if df['Outcome'][row] in bad_outcomes:
            landing_class.append(0)
        else:
            landing_class.append(1)
-------------------------------
Exploratory Data Analysis Overview
    Exploratory Data Analysis is the first step of any data science project. 
    I would suggest that considering what you'd like to accomplish and which data you want to examine to answer that first. But that's just me.
    In the lab, you will determine what attributes are correlated with successful landings. The categorical variables will be converted using one hot encoding, preparing the data for a machine learning model that will predict if the first stage will successfully land.
-----------------------
Test
What I created in the notebook:
    sns.catplot(y="LaunchSite", x="FlightNumber", hue="Class", data=df, aspect = 5)
    plt.xlabel("Flight Number",fontsize=20)
    plt.ylabel("Launch Site",fontsize=20)
    plt.show()    
a)     
-------------------------------
Interactive Visual Analytics and Dashboards
    Oh shit. I couldn't get this to work last time. And it took forever.
    Folium and Plotly Dash.
    Data is Launch Site Geo data with Folium. 
    How to choose the optimum launch site.
-------------------------
Now I'm stuck on trying to pull the lat/long values from the data frame for each of the coordinate sets. Look in the example notebook from Data Visualization with Python course.
    incidents = folium.map.FeatureGroup()
    for lat, lng, in zip(df_incidents.Y, df_incidents.X):
    incidents.add_child(
        folium.features.CircleMarker(
            [lat, lng],
            radius=5, # define how big you want the circle markers to be
            color='yellow',
            fill=True,
            fill_color='blue',
            fill_opacity=0.6
        )
    )
    sanfran_map.add_child(incidents)
Figured it out, sort of:

    site_map = folium.Map(location=nasa_coordinate, zoom_start=5)
    incidents = folium.map.FeatureGroup()
    for lat,long,site in zip(launch_sites_df.Lat, launch_sites_df.Long, launch_sites_df['Launch Site']):
        print(lat,long,site)
        incidents.add_child(
            folium.features.Circle(
                [lat, long],
                radius=2000, # define how big you want the circle markers to be
                color='yellow',
                fill=True,
                fill_color='blue',
                fill_opacity=0.6,
            ).add_child(folium.Popup(site))
        )
        incidents.add_child(
            folium.features.Marker(
                [lat, long],
                icon=DivIcon(icon_size=(20,20),icon_anchor=(0,0), html='<div style="font-size: 12; color:#d35400;"><b>%s</b></div>' % site, )
            )
        )
site_map.add_child(incidents)
--------------
Another way to iterate over a dataframe:
    df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})

    for index, row in df.iterrows():
        print(row['c1'], row['c2'])
    for index, row in spacex_df.iterrows():
        print(row['Lat'], row['Long'], row['marker_color'])
        prints a row of three variables by column name specified.
How someone else did it:
    site_map.add_child(marker_cluster) # marker_cluster was created as an instance of the Folium object MarkerCluster which has magic powers in maps
    for index, row in spacex_df.iterrows(): # for every row on spacex_df, assign the index of the frame to index, and assign to row the ability to pick column contents by indexing them by column name.
        folium.map.Marker(
            (row['Lat'], row['Long']), # the coordinates in the frame are in columns Lat and Long 
            icon=folium.Icon(color='white', # I don't know what that does 
            icon_color= row['marker_color'])).add_to(marker_cluster) # this is the magic of the object MarkerCluster

        site_map.add_child(marker_cluster)
    site_map 
Amazing.    
------------------------
Write down two coordinates, Vandeberg AFB and Los Alamos
VBerg: 34.63208 -120.61478
LAM: 34.74584 -120.17498
COast: 34.63603 -120.62405
Lompoc: 34.64168, -120.45067
Santa Maria: 34.94393, -120.43213
---------------------------
Graded Quiz - did pretty well. I would have been lost without seeing how that other guy did it.
Nishit-purbia IBM-Applied-data-Science-Capstone-project git project is awesome.
https://github.com/Nishit-purbia/IBM-Applied-data-Science-Capstone-project
---------------------------------------------
https://www.coursera.org/learn/applied-data-science-capstone/lecture/NtL9F/predictive-analysis-overview
--------------------------------------
----------------------------
Predictive Analysis Overview     
    In this lab, we will build a machine learning pipeline to predict if the first stage of the Falcon 9 lands successfully. 
    This will include: 
        Preprocessing allowing us to standardize our data
        Train_test_split allowing us to split our data 
        We will train the model
        Grid Search on each algorithm to find the hyperparameters for optimum performance
        Determine the model with the best accuracy using the training data. You will test:
            Logistic Regression
            Support Vector machines
            Decision Tree Classifier
            K-nearest neighbors
        Finally, we will output the confusion matrix.

Grid search - first up is logistic regression grid search
    parameters ={"C":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}
    lr=LogisticRegression()
    grid_search = GridSearchCV(lr, parameters, cv=10)
    logreg_cv = grid_search.fit(X_train, Y_train)        

    print("tuned hyperparameters: ",logreg_cv.best_params_)
    print("accuracy :",logreg_cv.best_score_)

    logreg_cv.score(X_test,Y_test)

    yhat=logreg_cv.predict(X_test)
    plot_confusion_matrix(Y_test,yhat)
-----------------
Elements Of A Successful Data Findings Report
    documents, slide show
    Before you start writing, understand the few important messages, and build your report around those.    
    outline formats
        cover page - title of the report, names of authors, their affiliations, and contacts, the name of the institutional publisher (if any), and the date of publication.
        executive summary - Ok to summarize other parts of the report, but do not include anything that is not discussed in detail elsewhere in the report. why the reader will gain from reading the complete report
        table of contents 
        introduction - nature of the analysis, statement of the problem, questions to be answered, the aims of the author, practicality of the work.
        methodology - data sources, plan for data, algorithms used, what is new about the methods, and others who have done similar work 
        results - what you learned about orgainizing and analyzing the data - includes charts and graphs 
        discussion - findings and implications. Maybe recommendations. 
        conclusion - summary of the findings, any future steps 
        appendix - things that didn't fit in the report but still important, links to raw data
------------------------------
Best Practices For Presenting Your Findings
    Graphs not too small and are well labeled.         
    Raw data is only supporting evidence, not the main communicating method
    Discuss only one point from each chart.
    If it doesn't support the argument, leave it out of the main report sections. Put it in the appendix or leave it out.
-----------
Final project:
    There are a total of 40 points possible for the final assessment, and you will be graded by your peers, who are also completing this assignment.

    The main grading criteria will be:

        Uploaded the URL of your GitHub repository including all the completed notebooks and Python files (1 pt)
        Uploaded your completed presentation in PDF format (1 pt)
        Completed the required Executive Summary slide (1 pt)
        Completed the required Introduction slide (1 pt)
        Completed the required data collection and data wrangling methodology related slides (1 pt)
        Completed the required EDA and interactive visual analytics methodology related slides (3 pts)
        Completed the required predictive analysis methodology related slides (1 pt)
        Completed the required EDA with visualization results slides (6 pts)
        Completed the required EDA with SQL results slides (10 pts)
        Completed the required interactive map with Folium results slides (3 pts)
        Completed the required Plotly Dash dashboard results slides (3 pts)
        Completed the required predictive analysis (classification) results slides (6 pts)
        Completed the required Conclusion slide (1 pts)
        Applied your creativity to improve the presentation beyond the template (1 pts)
        Displayed any innovative insights (1 pts)
-------------------------
Key points for the exec summary:
Outcome meanings:
    True Ocean means the mission outcome was successfully landed to a specific region of the ocean while 
    False Ocean means the mission outcome was unsuccessfully landed to a specific region of the ocean. 
    True RTLS means the mission outcome was successfully landed to a ground pad 
    False RTLS means the mission outcome was unsuccessfully landed to a ground pad. 
    True ASDS means the mission outcome was successfully landed to a drone ship 
    False ASDS means the mission outcome was unsuccessfully landed to a drone ship. 
    None ASDS and None None these represent a failure to land.
    Results:
        True ASDS      41   successfully landed to a drone ship 
        None None      19   represent a failure to land
        True RTLS      14   sucessfully landed to a ground pad 
        False ASDS      6   unsuccessfully landed to a drone ship. 
        True Ocean      5   successfully landed to a specific region of the ocean
        False Ocean     2   unsuccessfully landed to a specific region of the ocean
        None ASDS       2   represent a failure to land
        False RTLS      1   unsuccessfully landed to a ground pad
    Class: bad outcomes:  {'False ASDS', 'None None', 'False RTLS', 'None ASDS', 'False Ocean'}
           good outcomes:  {'True RTLS', 'None ASDS', 'True Ocean', 'True ASDS'}
        Good:    60
        Bad:    30
    Launch successes:
        true: 131
        false: 5